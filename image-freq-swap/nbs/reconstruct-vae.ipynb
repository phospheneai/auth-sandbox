{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9685ee8-fee2-45bb-b084-3047b8adfffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/uvieea/miniconda3/envs/authgenimage/lib/python3.12/site-packages/torch/amp/autocast_mode.py:266: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from diffusers import AutoencoderKL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "62b0a306-07fc-4b07-a856-bb7c8fd21bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Reconstructor():\n",
    "    def __init__(self,device,use_fp16,*args,**kwargs):\n",
    "        self.device = device if device else (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.use_fp16 = use_fp16 and \"cuda\" in self.device\n",
    "        self.dtype = torch.float16 if self.use_fp16 else torch.float32\n",
    "\n",
    "    def crop_by_eight(self,img):\n",
    "        w, h = img.size\n",
    "        new_w, new_h = (w // 8) * 8, (h // 8) * 8\n",
    "        if new_w != w or new_h != h:\n",
    "            img = img.crop(((w-new_w)//2, (h-new_h)//2, (w-new_w)//2 + new_w, (h-new_h)//2 + new_h))\n",
    "        return img \n",
    "\n",
    "    def preprocess(self):\n",
    "        pass\n",
    "        \n",
    "    def reconstruct(self):\n",
    "        pass\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d63f67d4-af1f-47ed-a16a-74f4b954d2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class VAEReconstructor(Reconstructor):\n",
    "    def __init__(self, model_id=\"stabilityai/sd-vae-ft-mse\", device=None,use_fp16=True,*args,**kwargs):\n",
    "        super().__init__(device,use_fp16,*args,**kwargs)\n",
    "        \n",
    "        print(f\"Loading VAE from '{model_id}' to {self.device}...\")\n",
    "        try:\n",
    "            self.vae = AutoencoderKL.from_pretrained(model_id, torch_dtype=self.dtype)\n",
    "        except OSError:\n",
    "            self.vae = AutoencoderKL.from_pretrained(model_id, subfolder=\"vae\", torch_dtype=self.dtype)\n",
    "            \n",
    "        self.vae.to(self.device)\n",
    "        self.vae.eval()\n",
    "\n",
    "    def preprocess(self, img):\n",
    "        # Crop to multiple of 8\n",
    "        img = self.crop_by_eight(img)\n",
    "        # Normalize to [-1, 1]\n",
    "        x = torch.from_numpy(np.array(img)).float() / 255.0\n",
    "        x = x.permute(2, 0, 1).unsqueeze(0)\n",
    "        return img, (2.0 * x - 1.0).to(self.device, dtype=self.dtype)\n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def reconstruct(self, img):\n",
    "        original_cropped, input_tensor = self.preprocess(img)\n",
    "        \n",
    "        latents = self.vae.encode(input_tensor).latent_dist.sample()\n",
    "        decoded = self.vae.decode(latents).sample\n",
    "\n",
    "        decoded = (decoded / 2 + 0.5).clamp(0, 1)\n",
    "        decoded = decoded.cpu().permute(0, 2, 3, 1).float().numpy()\n",
    "        decoded = (decoded * 255).round().astype(\"uint8\")[0]\n",
    "\n",
    "        return original_cropped, Image.fromarray(decoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "68ee6d3d-b914-4ee5-b4f6-aa4922d95586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading VAE from 'stabilityai/sd-vae-ft-mse' to cpu...\n"
     ]
    }
   ],
   "source": [
    "reconstructor = VAEReconstructor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "55b65a9a-bb86-4bab-8686-66ee245627cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "img = Image.open(\"../../../../data/generated-image-detection/train/real/AADB_newtest/0.050_farm1_269_19943988589_646a5d1dda_b.jpg\")\n",
    "real_img, recon_img = reconstructor.reconstruct(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ec0273f4-7eae-4617-be43-ee0ef0e38a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_img.save(\"../src/real.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "25dd173e-b5ee-4eeb-8d8c-4f22de1435f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "recon_img.save(\"../src/recon.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "61f889b3-8626-4e5b-bbf5-a70f44d85648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import numpy as np\n",
    "# from PIL import Image\n",
    "# from diffusers import AutoencoderKL, VQModel\n",
    "\n",
    "# # --- 1. SDXL Reconstructor (Handles the FP16 'Black Image' Bug) ---\n",
    "# class SDXLReconstructor(Reconstructor):\n",
    "#     def __init__(self, model_id=\"stabilityai/sdxl-vae\", device=None, use_fp16=True, *args, **kwargs):\n",
    "#         # SDXL TRAP: The original 'stabilityai/sdxl-vae' often outputs NaNs (black images) in FP16.\n",
    "#         # If the user requests FP16, we automatically switch to the community-standard fix.\n",
    "#         if use_fp16 and model_id == \"stabilityai/sdxl-vae\":\n",
    "#             print(\"Warning: Switching to 'madebyollin/sdxl-vae-fp16-fix' to prevent NaNs in FP16.\")\n",
    "#             model_id = \"madebyollin/sdxl-vae-fp16-fix\"\n",
    "            \n",
    "#         super().__init__(device, use_fp16, *args, **kwargs)\n",
    "        \n",
    "#         print(f\"Loading SDXL VAE from '{model_id}'...\")\n",
    "#         self.vae = AutoencoderKL.from_pretrained(model_id, torch_dtype=self.dtype)\n",
    "#         self.vae.to(self.device)\n",
    "#         self.vae.eval()\n",
    "\n",
    "#     def preprocess(self, img):\n",
    "#         # SDXL uses the same preprocessing as SD1.5 ([-1, 1] range)\n",
    "#         img = self.crop_by_eight(img)\n",
    "#         x = torch.from_numpy(np.array(img)).float() / 255.0\n",
    "#         x = x.permute(2, 0, 1).unsqueeze(0)\n",
    "#         return img, (2.0 * x - 1.0).to(self.device, dtype=self.dtype)\n",
    "\n",
    "#     @torch.no_grad()\n",
    "#     def reconstruct(self, img):\n",
    "#         original_cropped, input_tensor = self.preprocess(img)\n",
    "        \n",
    "#         # SDXL Encoder works the same as standard KL\n",
    "#         latents = self.vae.encode(input_tensor).latent_dist.sample()\n",
    "        \n",
    "#         # Decoding\n",
    "#         decoded = self.vae.decode(latents).sample\n",
    "\n",
    "#         # Post-process (Standard clamping)\n",
    "#         decoded = (decoded / 2 + 0.5).clamp(0, 1)\n",
    "#         decoded = decoded.cpu().permute(0, 2, 3, 1).float().numpy()\n",
    "#         decoded = (decoded * 255).round().astype(\"uint8\")[0]\n",
    "\n",
    "#         return original_cropped, Image.fromarray(decoded)\n",
    "\n",
    "\n",
    "# # --- 2. VQ-GAN Reconstructor (Handles Discrete Vector Quantization) ---\n",
    "# class VQReconstructor(Reconstructor):\n",
    "#     def __init__(self, model_id=\"microsoft/vq-diffusion-ithq\", device=None, use_fp16=True, *args, **kwargs):\n",
    "#         super().__init__(device, use_fp16, *args, **kwargs)\n",
    "        \n",
    "#         print(f\"Loading VQModel from '{model_id}'...\")\n",
    "#         # Note: We use VQModel class here, not AutoencoderKL\n",
    "#         self.vae = VQModel.from_pretrained(model_id, subfolder=\"vqvae\", torch_dtype=self.dtype)\n",
    "#         self.vae.to(self.device)\n",
    "#         self.vae.eval()\n",
    "\n",
    "#     def preprocess(self, img):\n",
    "#         # VQ-GANs usually behave better with standard [-1, 1] scaling\n",
    "#         img = self.crop_by_eight(img)\n",
    "#         x = torch.from_numpy(np.array(img)).float() / 255.0\n",
    "#         x = x.permute(2, 0, 1).unsqueeze(0)\n",
    "#         return img, (2.0 * x - 1.0).to(self.device, dtype=self.dtype)\n",
    "\n",
    "#     @torch.no_grad()\n",
    "#     def reconstruct(self, img):\n",
    "#         original_cropped, input_tensor = self.preprocess(img)\n",
    "        \n",
    "#         # DIFFERENCE: VQModel encoder output is not a distribution (.sample()), \n",
    "#         # it is a direct latent representation (often .latents)\n",
    "#         encoded_output = self.vae.encode(input_tensor)\n",
    "        \n",
    "#         # Some VQ implementations return a tuple or object, handle specifically:\n",
    "#         if hasattr(encoded_output, \"latents\"):\n",
    "#             latents = encoded_output.latents\n",
    "#         else:\n",
    "#             latents = encoded_output # Fallback for some older diffusers versions\n",
    "\n",
    "#         decoded = self.vae.decode(latents).sample\n",
    "\n",
    "#         decoded = (decoded / 2 + 0.5).clamp(0, 1)\n",
    "#         decoded = decoded.cpu().permute(0, 2, 3, 1).float().numpy()\n",
    "#         decoded = (decoded * 255).round().astype(\"uint8\")[0]\n",
    "\n",
    "#         return original_cropped, Image.fromarray(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5a7ce3c9-480c-40fc-b1f9-929d388aea54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reconstructor = VQReconstructor()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
